
# 基于眼动追踪的HoloLens 2与机械臂协同控制系统 (第一阶段)

## 项目简介

本项目是一个基于混合现实（Mixed Reality）和计算机视觉的创新性人机交互系统。它旨在允许用户通过HoloLens 2设备，仅使用**眼动追踪**作为输入方式，来直观、高效地识别真实世界中的物体，并向一个（模拟的）机械臂下达精确的操作指令。

第一阶段的核心目标是搭建并验证整个系统的技术架构和核心交互逻辑，为后续集成真实机械臂和引入SSVEP（稳态视觉诱发电位）等更高级的脑机接口技术奠定坚实的基础。

---

## 功能特性 (第一阶段成果)

* **实时物体检测与追踪**: PC端服务器使用YOLO11模型，能够实时检测和追踪摄像头视野内的多个物体，并为每个物体分配一个唯一的ID。
* **第一人称视角视频流**: PC端的摄像头画面（模拟HoloLens 2的第一人称视角）被实时压缩并通过UDP协议串流至Unity客户端，在HoloLens 2中以UI面板的形式呈现。
* **双向指令通信**: 使用TCP协议在PC端和Unity端之间建立了稳定、可靠的双向通信渠道，用于传输物体数据、用户选择、系统状态和操作指令。
* **动态UI生成与交互**: Unity端能够根据PC端发来的物体数据，在视频流画面的对应位置动态生成可交互的3D标记（Marker）。
* **纯眼动交互**: 用户通过注视（Gaze Dwell）动态生成的物体标记或UI功能按钮，即可完成**目标选择**和**指令下达**，全程无需手势或语音。
* **严谨的状态机逻辑**: 整个系统由一个定义清晰的状态机驱动，确保了在“空闲检测”、“等待指令”、“移动模式”、“指令执行”等不同状态间流畅、无冲突地切换。
* **视觉刺激信号基础**: 动态生成的标记物具备可控的闪烁功能，为第二阶段集成SSVEP脑机接口技术提供了视觉刺激信号的基础。

---

## 技术架构

本系统采用客户端/服务器（C/S）架构，由PC端服务器和HoloLens 2端的Unity客户端两部分组成。

### PC端 (Python 服务器)

作为系统的大脑，负责所有的计算密集型任务。

* **核心语言**: Python 3
* **主要库**:
    * `pyrealsense2`: 用于从Intel RealSense相机捕获彩色和深度图像流。
    * `ultralytics (YOLO11)`: 用于执行高效的物体检测和追踪。
    * `OpenCV (cv2)`: 用于图像处理和在本地窗口进行调试预览。
    * `socket`, `threading`: 用于实现TCP/UDP服务器和处理耗时任务的后台线程。
* **职责**:
    1.  管理和驱动状态机（State Machine），控制整个应用的逻辑流。
    2.  运行YOLO模型，处理视频帧。
    3.  通过UDP向Unity发送实时视频流。
    4.  通过TCP与Unity进行结构化数据（JSON）的收发。
    5.  模拟执行机械臂的耗时任务。

### HoloLens 2端 (Unity 客户端)

作为系统的交互界面，负责所有与用户和混合现实环境相关的任务。

* **核心引擎**: Unity
* **关键插件**: Microsoft Mixed Reality Toolkit (MRTK)
* **核心语言**: C#
* **职责**:
    1.  接收UDP视频流并将其渲染到用户眼前的UI面板（`RawImage`）上。
    2.  作为TCP客户端，与PC服务器保持长连接。
    3.  解析来自PC的JSON数据，如`object_added`, `subtitle`等。
    4.  根据物体坐标，在UI上动态实例化、更新和销毁3D标记（`markerPrefab`）。
    5.  利用MRTK的眼动追踪API (`EyeGazeProvider`) 实现注视检测。
    6.  处理注视悬停（Gaze Dwell）事件，并将用户的选择（`selection`）或指令（`command`）发送回PC。
    7.  管理UI状态（如按钮的激活/禁用，字幕的显示等）。

---

## 核心工作流（状态机逻辑）

1.  **状态1: 空闲/侦测中 (Idle/Detecting)**
    * PC端持续检测物体，并将新增、移动或消失的物体信息发送给Unity。
    * Unity根据信息，在画面上动态生成、移动或销毁闪烁的物体标记。
    * 用户通过**注视**其中一个闪烁标记来选择目标物体。

2.  **状态2: 等待指令 (Awaiting Command)**
    * 当用户完成选择后，Unity将选中物体的ID发回PC。
    * PC端确认收到，状态切换，并向Unity发送字幕提示，同时Unity激活所有功能按钮（吃、抓、移动等）。
    * 用户通过**注视**某个功能按钮来下达指令。

3.  **状态3: “移动”模式 (Move Mode)**
    * 如果用户选择了“移动”指令，PC端进入此特殊模式。
    * PC生成一个九宫格坐标点列表并发送给Unity。
    * Unity在画面上生成九个闪烁的目标点标记。
    * 用户通过**注视**其中一个目标点来确定移动的终点。

4.  **状态4: 指令执行中 (Command Executing)**
    * PC端收到最终指令后，进入此状态。
    * PC在一个独立的后台线程中开始执行（模拟的）耗时机械臂任务。
    * 在执行期间，所有UI交互被锁定，Unity只显示“正在执行...”的字幕。
    * 任务完成后，PC通知Unity，整个系统**状态切换回状态1**，开始新一轮的交互。

---

## 未来展望 (第二阶段)

* **集成真实机械臂**: 将`execute_robot_arm_task`中的模拟代码替换为与真实机械臂（如UR、Franka等）的控制SDK或ROS节点的通信。
* **实现SSVEP**: 利用已有的闪烁标记，为每个标记设置不同的闪烁频率。在HoloLens端集成脑电信号采集设备（如MNE），并编写算法来解码用户的意图，实现更直接的“意念”选择。
* **UI/UX优化**: 优化UI布局、反馈效果和交互流程，提升用户体验。